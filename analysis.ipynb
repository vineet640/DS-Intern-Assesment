{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. What relevant key metrics are provided to evaluate the CTA combinations? And which CTA Copy and CTA Placement did best/worst based on the key metrics? - The main metric provided to evaluate the CTA combinations is click through rate (CTR). This is because the higher the CTR, the more likely the user will click on the CTA and visit the website, which means that this would allow us to evaluate the CTA combinations. Other key metrics are submittedForm, scheduledAppointment, and revenue as these also allow us to evaluate the CTA combinations in terms of what types of clicks happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = train_df.groupby(['ctaCopy', 'ctaPlacement']).agg({\n",
        "    'clickedCTA': 'mean',\n",
        "    'submittedForm': 'mean',\n",
        "    'scheduledAppointment': 'mean',\n",
        "    'revenue': 'mean'\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Displaying Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                      ctaCopy ctaPlacement  clickedCTA  submittedForm  scheduledAppointment    revenue\n",
            "                  Access Your Personalized Mortgage Rates Now       Bottom    0.134821       0.117001              0.051751 218.982609\n",
            "                  Access Your Personalized Mortgage Rates Now       Middle    0.161462       0.126901              0.050671 225.461812\n",
            "                  Access Your Personalized Mortgage Rates Now          Top    0.186482       0.150752              0.054631 221.869852\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate       Bottom    0.153092       0.135631              0.056881 226.882911\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate       Middle    0.169922       0.135811              0.053191 226.945854\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate          Top    0.198452       0.159032              0.054541 225.280528\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes       Bottom    0.154172       0.150122              0.056701 206.301587\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes       Middle    0.183332       0.164792              0.057871 203.102644\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes          Top    0.211753       0.190875              0.060295 210.313433\n"
          ]
        }
      ],
      "source": [
        "print(metrics[['ctaCopy', 'ctaPlacement', 'clickedCTA', 'submittedForm', 'scheduledAppointment', 'revenue']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Performing Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest clickedCTA: Get Pre-Approved for a Mortgage in 5 Minutes - Top (0.2118)\n",
            "Highest submittedForm: Get Pre-Approved for a Mortgage in 5 Minutes - Top (0.1909)\n",
            "Highest scheduledAppointment: Get Pre-Approved for a Mortgage in 5 Minutes - Top (0.0603)\n",
            "Highest Revenue: First Time? We've Made it Easy to Find the Best Mortgage Rate - Middle ($226.95)\n"
          ]
        }
      ],
      "source": [
        "best_clicked = metrics.loc[metrics['clickedCTA'].idxmax()]\n",
        "best_submitted = metrics.loc[metrics['submittedForm'].idxmax()]\n",
        "best_appointment = metrics.loc[metrics['scheduledAppointment'].idxmax()]\n",
        "best_revenue = metrics.loc[metrics['revenue'].idxmax()]\n",
        "\n",
        "print(f\"Highest clickedCTA: {best_clicked['ctaCopy']} - {best_clicked['ctaPlacement']} ({best_clicked['clickedCTA']:.4f})\")\n",
        "print(f\"Highest submittedForm: {best_submitted['ctaCopy']} - {best_submitted['ctaPlacement']} ({best_submitted['submittedForm']:.4f})\")\n",
        "print(f\"Highest scheduledAppointment: {best_appointment['ctaCopy']} - {best_appointment['ctaPlacement']} ({best_appointment['scheduledAppointment']:.4f})\")\n",
        "print(f\"Highest Revenue: {best_revenue['ctaCopy']} - {best_revenue['ctaPlacement']} (${best_revenue['revenue']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst Performing Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lowest clickedCTA: Access Your Personalized Mortgage Rates Now - Bottom (0.1348)\n",
            "Lowest submittedForm: Access Your Personalized Mortgage Rates Now - Bottom (0.1170)\n",
            "Lowest scheduledAppointment: Access Your Personalized Mortgage Rates Now - Middle (0.0507)\n",
            "Lowest Revenue: Get Pre-Approved for a Mortgage in 5 Minutes - Middle ($203.10)\n"
          ]
        }
      ],
      "source": [
        "worst_clicked = metrics.loc[metrics['clickedCTA'].idxmin()]\n",
        "worst_submitted = metrics.loc[metrics['submittedForm'].idxmin()]\n",
        "worst_appointment = metrics.loc[metrics['scheduledAppointment'].idxmin()]\n",
        "worst_revenue = metrics.loc[metrics['revenue'].idxmin()]\n",
        "\n",
        "print(f\"Lowest clickedCTA: {worst_clicked['ctaCopy']} - {worst_clicked['ctaPlacement']} ({worst_clicked['clickedCTA']:.4f})\")\n",
        "print(f\"Lowest submittedForm: {worst_submitted['ctaCopy']} - {worst_submitted['ctaPlacement']} ({worst_submitted['submittedForm']:.4f})\")\n",
        "print(f\"Lowest scheduledAppointment: {worst_appointment['ctaCopy']} - {worst_appointment['ctaPlacement']} ({worst_appointment['scheduledAppointment']:.4f})\")\n",
        "print(f\"Lowest Revenue: {worst_revenue['ctaCopy']} - {worst_revenue['ctaPlacement']} (${worst_revenue['revenue']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Which groups of people tend to be more correlated or less correlated with our key metrics?\n",
        "\n",
        "3. What ways can you manipulate the columns/dataset to create features that increase predictive power towards our key metric?\n",
        "\n",
        "4. Besides Log Loss, what other metrics will you use to evaluate the model's performance, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"Load train and test data from current directory.\"\"\"\n",
        "    train_path = 'train.csv'\n",
        "    test_path = 'test.csv'\n",
        "    \n",
        "    if not os.path.exists(train_path):\n",
        "        raise FileNotFoundError(f\"Training file '{train_path}' not found\")\n",
        "    if not os.path.exists(test_path):\n",
        "        raise FileNotFoundError(f\"Test file '{test_path}' not found\")\n",
        "    \n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    \n",
        "    print(f\"Loaded train data: {train_df.shape}\")\n",
        "    print(f\"Loaded test data: {test_df.shape}\")\n",
        "    \n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Building Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Build the preprocessing and modeling pipeline.\"\"\"\n",
        "    \n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=transformers,\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=None))\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred_proba):\n",
        "    \"\"\"Calculate and print evaluation metrics.\"\"\"\n",
        "    logloss = log_loss(y_true, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    brier = brier_score_loss(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"Log Loss: {logloss:.6f} | ROC-AUC: {roc_auc:.6f} | Brier Score: {brier:.6f}\")\n",
        "    return logloss, roc_auc, brier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction and Saving Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_and_save(pipeline, X_test, test_df_original, name=\"Vineet_Burugu\"):\n",
        "    \"\"\"Generate predictions and save to CSV.\"\"\"\n",
        "    os.makedirs('./outputs', exist_ok=True)\n",
        "    output_path = f'./outputs/{name}_predictions.csv'\n",
        "    \n",
        "    pr_CTA = pipeline.predict_proba(X_test)[:, 1]\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'userId': test_df_original['userId'].values,\n",
        "        'pr_CTA': pr_CTA\n",
        "    })\n",
        "    \n",
        "    predictions_df.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions saved to: {output_path}\")\n",
        "    return predictions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data: (100000, 18)\n",
            "Loaded test data: (20000, 17)\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df = load_data()\n",
        "\n",
        "feature_cols = [\n",
        "    'ctaCopy', 'ctaPlacement', 'sessionReferrer', 'browser', \n",
        "    'deviceType', 'estimatedAnnualIncome', 'estimatedPropertyType', \n",
        "    'visitCount', 'pageURL', 'scrollDepth', 'editorialSnippet'\n",
        "]\n",
        "\n",
        "available_features = [col for col in feature_cols if col in train_df.columns]\n",
        "\n",
        "X_train = train_df[available_features].copy()\n",
        "y_train = train_df['clickedCTA'].copy()\n",
        "X_test = test_df[available_features].copy()\n",
        "\n",
        "if 'editorialSnippet' in X_train.columns:\n",
        "    X_train['editorialSnippet'] = X_train['editorialSnippet'].astype(str).str.len()\n",
        "    X_test['editorialSnippet'] = X_test['editorialSnippet'].astype(str).str.len()\n",
        "\n",
        "categorical_features = [f for f in ['ctaCopy', 'ctaPlacement', 'sessionReferrer', 'browser', \n",
        "                                    'deviceType', 'estimatedPropertyType', 'pageURL'] \n",
        "                        if f in available_features]\n",
        "\n",
        "numeric_features = [f for f in ['estimatedAnnualIncome', 'visitCount', 'scrollDepth'] \n",
        "                    if f in available_features]\n",
        "\n",
        "if 'editorialSnippet' in available_features:\n",
        "    numeric_features.append('editorialSnippet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Loss: 0.422296 | ROC-AUC: 0.700934 | Brier Score: 0.133458\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4222961053243016, 0.7009335232790259, 0.13345798393221112)"
            ]
          },
          "execution_count": 208,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline = build_pipeline(categorical_features, numeric_features)\n",
        "pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "y_val_pred_proba = pipeline.predict_proba(X_val_split)[:, 1]\n",
        "evaluate(y_val_split, y_val_pred_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to: ./outputs/Vineet_Burugu_predictions.csv\n",
            "Prediction range: [0.006222, 0.612568]\n"
          ]
        }
      ],
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "predictions_df = predict_and_save(pipeline, X_test, test_df, name=\"Vineet_Burugu\")\n",
        "print(f\"Prediction range: [{predictions_df['pr_CTA'].min():.6f}, {predictions_df['pr_CTA'].max():.6f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Iteration 1: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Model Comparison: Logistic Regression vs HistGradientBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "def build_tree_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Build pipeline for HistGradientBoostingClassifier (no scaling needed).\"\"\"\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median'))\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=transformers,\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', HistGradientBoostingClassifier(\n",
        "            max_iter=200,\n",
        "            early_stopping=True,\n",
        "            validation_fraction=0.1,\n",
        "            n_iter_no_change=10,\n",
        "            random_state=42,\n",
        "            class_weight=None\n",
        "        ))\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Comparing Both Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n",
            "Training HistGradientBoostingClassifier...\n",
            "\n",
            "============================================================\n",
            "VALIDATION METRICS COMPARISON\n",
            "============================================================\n",
            "\n",
            "Logistic Regression:\n",
            "Log Loss: 0.422296 | ROC-AUC: 0.700934 | Brier Score: 0.133458\n",
            "\n",
            "HistGradientBoostingClassifier:\n",
            "Log Loss: 0.385112 | ROC-AUC: 0.761368 | Brier Score: 0.125093\n",
            "\n",
            "============================================================\n",
            "Winner: HistGradientBoostingClassifier (Log Loss: 0.385112)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "logistic_pipeline = build_pipeline(categorical_features, numeric_features)\n",
        "tree_pipeline = build_tree_pipeline(categorical_features, numeric_features)\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "logistic_pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "print(\"Training HistGradientBoostingClassifier...\")\n",
        "tree_pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION METRICS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_val_logistic = logistic_pipeline.predict_proba(X_val_split)[:, 1]\n",
        "y_val_tree = tree_pipeline.predict_proba(X_val_split)[:, 1]\n",
        "\n",
        "print(\"\\nLogistic Regression:\")\n",
        "log_loss_lr, roc_auc_lr, brier_lr = evaluate(y_val_split, y_val_logistic)\n",
        "\n",
        "print(\"\\nHistGradientBoostingClassifier:\")\n",
        "log_loss_tree, roc_auc_tree, brier_tree = evaluate(y_val_split, y_val_tree)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if log_loss_lr < log_loss_tree:\n",
        "    winner = \"Logistic Regression\"\n",
        "    winner_pipeline = logistic_pipeline\n",
        "    print(f\"Winner: {winner} (Log Loss: {log_loss_lr:.6f})\")\n",
        "else:\n",
        "    winner = \"HistGradientBoostingClassifier\"\n",
        "    winner_pipeline = tree_pipeline\n",
        "    print(f\"Winner: {winner} (Log Loss: {log_loss_tree:.6f})\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Iteration 2: Model Improvement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Select a Machine Learning Algorithm\n",
        "\n",
        "Choose the most suitable algorithm based on the problem type, data characteristics, and performance requirements.\n",
        "\n",
        "We'll compare multiple algorithms:\n",
        "- **Linear Models**: Logistic Regression (fast, interpretable)\n",
        "- **Tree-based**: Random Forest, HistGradientBoostingClassifier (good for non-linear patterns)\n",
        "- **Distance-based**: KNN (simple, can capture local patterns)\n",
        "- **Neural Networks**: MLPClassifier (can capture complex interactions)\n",
        "\n",
        "Considerations:\n",
        "- **Speed**: Linear models are fastest, neural networks slowest\n",
        "- **Accuracy**: Tree-based models often perform well on tabular data\n",
        "- **Interpretability**: Linear models are most interpretable\n",
        "- **Scalability**: Tree-based models scale well to large datasets\n",
        "- **Dataset size**: With 100K samples, we can use more complex models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "he sec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Import Additional Models\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import time\n",
        "\n",
        "print(\"Imported additional models for comparison\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Build Pipeline Functions for Different Models\n",
        "\n",
        "def build_rf_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Random Forest pipeline.\"\"\"\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median'))\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "def build_knn_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"KNN pipeline (requires scaling).\"\"\"\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', KNeighborsClassifier(n_neighbors=5, n_jobs=-1))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "def build_mlp_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Neural Network pipeline.\"\"\"\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(transformers=transformers, remainder='drop')\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, \n",
        "                                     random_state=42, early_stopping=True, validation_fraction=0.1))\n",
        "    ])\n",
        "    return pipeline\n",
        "\n",
        "print(\"Pipeline functions created for all models\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 8. Train the Model\n",
        "\n",
        "Train all selected algorithms on the prepared training data with engineered features. Monitor training progress and detect issues early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Train All Models with Engineered Features\n",
        "\n",
        "# Use the engineered features from section 5\n",
        "X_train_eng = create_engineered_features(X_train)\n",
        "X_test_eng = create_engineered_features(X_test)\n",
        "X_train_split_eng = create_engineered_features(X_train_split)\n",
        "X_val_split_eng = create_engineered_features(X_val_split)\n",
        "\n",
        "# Build all pipelines\n",
        "models = {\n",
        "    'Logistic Regression': build_pipeline(categorical_features_eng, numeric_features_eng),\n",
        "    'HistGradientBoosting': build_tree_pipeline(categorical_features_eng, numeric_features_eng),\n",
        "    'Random Forest': build_rf_pipeline(categorical_features_eng, numeric_features_eng),\n",
        "    'KNN': build_knn_pipeline(categorical_features_eng, numeric_features_eng),\n",
        "    'Neural Network (MLP)': build_mlp_pipeline(categorical_features_eng, numeric_features_eng)\n",
        "}\n",
        "\n",
        "# Train all models and track training time\n",
        "training_results = {}\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING ALL MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, pipeline in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    start_time = time.time()\n",
        "    try:\n",
        "        pipeline.fit(X_train_split_eng, y_train_split)\n",
        "        train_time = time.time() - start_time\n",
        "        training_results[name] = {\n",
        "            'pipeline': pipeline,\n",
        "            'train_time': train_time,\n",
        "            'status': 'success'\n",
        "        }\n",
        "        print(f\"  ✓ Completed in {train_time:.2f} seconds\")\n",
        "    except Exception as e:\n",
        "        training_results[name] = {\n",
        "            'pipeline': None,\n",
        "            'train_time': None,\n",
        "            'status': f'failed: {str(e)}'\n",
        "        }\n",
        "        print(f\"  ✗ Failed: {str(e)}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 9. Evaluate Model Performance\n",
        "\n",
        "Test all trained models on unseen validation data to measure generalization and reliability. Use multiple metrics and compare training vs validation results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Comprehensive Model Evaluation\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "\n",
        "def comprehensive_evaluate(y_true, y_pred_proba, y_pred=None):\n",
        "    \"\"\"Calculate comprehensive evaluation metrics.\"\"\"\n",
        "    if y_pred is None:\n",
        "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
        "    \n",
        "    metrics = {\n",
        "        'log_loss': log_loss(y_true, y_pred_proba),\n",
        "        'roc_auc': roc_auc_score(y_true, y_pred_proba),\n",
        "        'brier_score': brier_score_loss(y_true, y_pred_proba),\n",
        "        'accuracy': accuracy_score(y_true, y_pred),\n",
        "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'f1_score': f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "# Evaluate all models\n",
        "evaluation_results = {}\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL EVALUATION ON VALIDATION SET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for name, result in training_results.items():\n",
        "    if result['status'] == 'success':\n",
        "        pipeline = result['pipeline']\n",
        "        try:\n",
        "            y_val_pred_proba = pipeline.predict_proba(X_val_split_eng)[:, 1]\n",
        "            y_val_pred = pipeline.predict(X_val_split_eng)\n",
        "            \n",
        "            # Get training metrics for comparison\n",
        "            y_train_pred_proba = pipeline.predict_proba(X_train_split_eng)[:, 1]\n",
        "            y_train_pred = pipeline.predict(X_train_split_eng)\n",
        "            \n",
        "            val_metrics = comprehensive_evaluate(y_val_split, y_val_pred_proba, y_val_pred)\n",
        "            train_metrics = comprehensive_evaluate(y_train_split, y_train_pred_proba, y_train_pred)\n",
        "            \n",
        "            evaluation_results[name] = {\n",
        "                'val_metrics': val_metrics,\n",
        "                'train_metrics': train_metrics,\n",
        "                'train_time': result['train_time']\n",
        "            }\n",
        "        except Exception as e:\n",
        "            evaluation_results[name] = {'error': str(e)}\n",
        "\n",
        "# Display results in a table\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VALIDATION METRICS SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "results_df = []\n",
        "for name, result in evaluation_results.items():\n",
        "    if 'error' not in result:\n",
        "        metrics = result['val_metrics']\n",
        "        results_df.append({\n",
        "            'Model': name,\n",
        "            'Log Loss': f\"{metrics['log_loss']:.6f}\",\n",
        "            'ROC-AUC': f\"{metrics['roc_auc']:.6f}\",\n",
        "            'Brier Score': f\"{metrics['brier_score']:.6f}\",\n",
        "            'Accuracy': f\"{metrics['accuracy']:.4f}\",\n",
        "            'Precision': f\"{metrics['precision']:.4f}\",\n",
        "            'Recall': f\"{metrics['recall']:.4f}\",\n",
        "            'F1-Score': f\"{metrics['f1_score']:.4f}\",\n",
        "            'Train Time (s)': f\"{result['train_time']:.2f}\"\n",
        "        })\n",
        "\n",
        "if results_df:\n",
        "    results_table = pd.DataFrame(results_df)\n",
        "    print(results_table.to_string(index=False))\n",
        "    \n",
        "    # Identify best model by log loss (primary metric)\n",
        "    best_model_name = min(evaluation_results.keys(), \n",
        "                         key=lambda x: evaluation_results[x]['val_metrics']['log_loss'] \n",
        "                         if 'error' not in evaluation_results[x] else float('inf'))\n",
        "    best_metrics = evaluation_results[best_model_name]['val_metrics']\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\"BEST MODEL: {best_model_name}\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Log Loss: {best_metrics['log_loss']:.6f}\")\n",
        "    print(f\"ROC-AUC: {best_metrics['roc_auc']:.6f}\")\n",
        "    print(f\"Brier Score: {best_metrics['brier_score']:.6f}\")\n",
        "    print(f\"Accuracy: {best_metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1-Score: {best_metrics['f1_score']:.4f}\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Check for Overfitting/Underfitting\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING vs VALIDATION COMPARISON (Overfitting Check)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison_df = []\n",
        "for name, result in evaluation_results.items():\n",
        "    if 'error' not in result:\n",
        "        train_ll = result['train_metrics']['log_loss']\n",
        "        val_ll = result['val_metrics']['log_loss']\n",
        "        diff = train_ll - val_ll\n",
        "        \n",
        "        comparison_df.append({\n",
        "            'Model': name,\n",
        "            'Train Log Loss': f\"{train_ll:.6f}\",\n",
        "            'Val Log Loss': f\"{val_ll:.6f}\",\n",
        "            'Difference': f\"{diff:.6f}\",\n",
        "            'Status': 'Overfitting' if diff < -0.01 else 'Underfitting' if diff > 0.01 else 'Good'\n",
        "        })\n",
        "\n",
        "if comparison_df:\n",
        "    comparison_table = pd.DataFrame(comparison_df)\n",
        "    print(comparison_table.to_string(index=False))\n",
        "    print(\"\\nNote: Negative difference suggests overfitting, positive suggests underfitting\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Confusion Matrix for Best Model\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"CONFUSION MATRIX: {best_model_name}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_pipeline = training_results[best_model_name]['pipeline']\n",
        "y_val_pred_best = best_pipeline.predict(X_val_split_eng)\n",
        "cm = confusion_matrix(y_val_split, y_val_pred_best)\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(f\"                Predicted\")\n",
        "print(f\"              Negative  Positive\")\n",
        "print(f\"Actual Negative   {cm[0,0]:6d}   {cm[0,1]:6d}\")\n",
        "print(f\"        Positive   {cm[1,0]:6d}   {cm[1,1]:6d}\")\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nTrue Negatives: {tn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"True Positives: {tp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10. Hyperparameter Tuning\n",
        "\n",
        "Optimize the model's hyperparameters to achieve higher accuracy, stability, and better generalization. Use Grid Search and Random Search with cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameter Tuning for Top Models\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Select top 2-3 models for hyperparameter tuning based on validation log loss\n",
        "sorted_models = sorted(evaluation_results.items(), \n",
        "                      key=lambda x: x[1]['val_metrics']['log_loss'] \n",
        "                      if 'error' not in x[1] else float('inf'))\n",
        "\n",
        "top_models_for_tuning = [name for name, _ in sorted_models[:3] if 'error' not in evaluation_results[name]]\n",
        "print(f\"Selected models for hyperparameter tuning: {top_models_for_tuning}\")\n",
        "\n",
        "tuned_results = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameter Tuning: HistGradientBoostingClassifier\n",
        "\n",
        "if 'HistGradientBoosting' in top_models_for_tuning:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"HYPERPARAMETER TUNING: HistGradientBoostingClassifier\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    # Build base pipeline\n",
        "    base_pipeline = build_tree_pipeline(categorical_features_eng, numeric_features_eng)\n",
        "    \n",
        "    # Define parameter grid\n",
        "    param_grid = {\n",
        "        'classifier__max_iter': [200, 300, 500],\n",
        "        'classifier__max_depth': [5, 10, 15, None],\n",
        "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
        "        'classifier__min_samples_leaf': [10, 20, 30]\n",
        "    }\n",
        "    \n",
        "    # Use RandomizedSearchCV for faster search (sample 20 combinations)\n",
        "    print(\"Running RandomizedSearchCV (20 iterations)...\")\n",
        "    grid_search = RandomizedSearchCV(\n",
        "        base_pipeline,\n",
        "        param_grid,\n",
        "        n_iter=20,\n",
        "        cv=3,\n",
        "        scoring='neg_log_loss',\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train_split_eng, y_train_split)\n",
        "    tuning_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nTuning completed in {tuning_time:.2f} seconds\")\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best CV score (neg_log_loss): {grid_search.best_score_:.6f}\")\n",
        "    \n",
        "    # Evaluate on validation set\n",
        "    best_hgb = grid_search.best_estimator_\n",
        "    y_val_pred_proba_tuned = best_hgb.predict_proba(X_val_split_eng)[:, 1]\n",
        "    tuned_metrics = comprehensive_evaluate(y_val_split, y_val_pred_proba_tuned)\n",
        "    \n",
        "    tuned_results['HistGradientBoosting'] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_cv_score': grid_search.best_score_,\n",
        "        'val_metrics': tuned_metrics,\n",
        "        'pipeline': best_hgb,\n",
        "        'tuning_time': tuning_time\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nValidation Log Loss (tuned): {tuned_metrics['log_loss']:.6f}\")\n",
        "    print(f\"Validation Log Loss (original): {evaluation_results['HistGradientBoosting']['val_metrics']['log_loss']:.6f}\")\n",
        "    improvement = evaluation_results['HistGradientBoosting']['val_metrics']['log_loss'] - tuned_metrics['log_loss']\n",
        "    print(f\"Improvement: {improvement:+.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameter Tuning: Random Forest\n",
        "\n",
        "if 'Random Forest' in top_models_for_tuning:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"HYPERPARAMETER TUNING: Random Forest\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    base_pipeline = build_rf_pipeline(categorical_features_eng, numeric_features_eng)\n",
        "    \n",
        "    param_grid = {\n",
        "        'classifier__n_estimators': [100, 200, 300],\n",
        "        'classifier__max_depth': [10, 20, 30, None],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4]\n",
        "    }\n",
        "    \n",
        "    print(\"Running RandomizedSearchCV (15 iterations)...\")\n",
        "    grid_search = RandomizedSearchCV(\n",
        "        base_pipeline,\n",
        "        param_grid,\n",
        "        n_iter=15,\n",
        "        cv=3,\n",
        "        scoring='neg_log_loss',\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train_split_eng, y_train_split)\n",
        "    tuning_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nTuning completed in {tuning_time:.2f} seconds\")\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best CV score (neg_log_loss): {grid_search.best_score_:.6f}\")\n",
        "    \n",
        "    best_rf = grid_search.best_estimator_\n",
        "    y_val_pred_proba_tuned = best_rf.predict_proba(X_val_split_eng)[:, 1]\n",
        "    tuned_metrics = comprehensive_evaluate(y_val_split, y_val_pred_proba_tuned)\n",
        "    \n",
        "    tuned_results['Random Forest'] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_cv_score': grid_search.best_score_,\n",
        "        'val_metrics': tuned_metrics,\n",
        "        'pipeline': best_rf,\n",
        "        'tuning_time': tuning_time\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nValidation Log Loss (tuned): {tuned_metrics['log_loss']:.6f}\")\n",
        "    print(f\"Validation Log Loss (original): {evaluation_results['Random Forest']['val_metrics']['log_loss']:.6f}\")\n",
        "    improvement = evaluation_results['Random Forest']['val_metrics']['log_loss'] - tuned_metrics['log_loss']\n",
        "    print(f\"Improvement: {improvement:+.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Hyperparameter Tuning: Logistic Regression\n",
        "\n",
        "if 'Logistic Regression' in top_models_for_tuning:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"HYPERPARAMETER TUNING: Logistic Regression\")\n",
        "    print(\"=\"*70)\n",
        "    \n",
        "    base_pipeline = build_pipeline(categorical_features_eng, numeric_features_eng)\n",
        "    \n",
        "    param_grid = {\n",
        "        'classifier__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n",
        "        'classifier__penalty': ['l1', 'l2'],\n",
        "        'classifier__solver': ['liblinear', 'lbfgs']\n",
        "    }\n",
        "    \n",
        "    print(\"Running GridSearchCV...\")\n",
        "    grid_search = GridSearchCV(\n",
        "        base_pipeline,\n",
        "        param_grid,\n",
        "        cv=3,\n",
        "        scoring='neg_log_loss',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    start_time = time.time()\n",
        "    grid_search.fit(X_train_split_eng, y_train_split)\n",
        "    tuning_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\nTuning completed in {tuning_time:.2f} seconds\")\n",
        "    print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best CV score (neg_log_loss): {grid_search.best_score_:.6f}\")\n",
        "    \n",
        "    best_lr = grid_search.best_estimator_\n",
        "    y_val_pred_proba_tuned = best_lr.predict_proba(X_val_split_eng)[:, 1]\n",
        "    tuned_metrics = comprehensive_evaluate(y_val_split, y_val_pred_proba_tuned)\n",
        "    \n",
        "    tuned_results['Logistic Regression'] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_cv_score': grid_search.best_score_,\n",
        "        'val_metrics': tuned_metrics,\n",
        "        'pipeline': best_lr,\n",
        "        'tuning_time': tuning_time\n",
        "    }\n",
        "    \n",
        "    print(f\"\\nValidation Log Loss (tuned): {tuned_metrics['log_loss']:.6f}\")\n",
        "    print(f\"Validation Log Loss (original): {evaluation_results['Logistic Regression']['val_metrics']['log_loss']:.6f}\")\n",
        "    improvement = evaluation_results['Logistic Regression']['val_metrics']['log_loss'] - tuned_metrics['log_loss']\n",
        "    print(f\"Improvement: {improvement:+.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Compare All Models (Original vs Tuned)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL MODEL COMPARISON: ALL MODELS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Combine original and tuned results\n",
        "all_final_results = []\n",
        "\n",
        "# Add original models\n",
        "for name, result in evaluation_results.items():\n",
        "    if 'error' not in result:\n",
        "        all_final_results.append({\n",
        "            'Model': name,\n",
        "            'Type': 'Original',\n",
        "            'Log Loss': result['val_metrics']['log_loss'],\n",
        "            'ROC-AUC': result['val_metrics']['roc_auc'],\n",
        "            'F1-Score': result['val_metrics']['f1_score'],\n",
        "            'Train Time (s)': result['train_time']\n",
        "        })\n",
        "\n",
        "# Add tuned models\n",
        "for name, result in tuned_results.items():\n",
        "    all_final_results.append({\n",
        "        'Model': name,\n",
        "        'Type': 'Tuned',\n",
        "        'Log Loss': result['val_metrics']['log_loss'],\n",
        "        'ROC-AUC': result['val_metrics']['roc_auc'],\n",
        "        'F1-Score': result['val_metrics']['f1_score'],\n",
        "        'Train Time (s)': result['tuning_time']\n",
        "    })\n",
        "\n",
        "final_comparison_df = pd.DataFrame(all_final_results)\n",
        "final_comparison_df = final_comparison_df.sort_values('Log Loss')\n",
        "\n",
        "print(\"\\n\" + final_comparison_df.to_string(index=False))\n",
        "\n",
        "# Identify overall best model\n",
        "best_final_model = final_comparison_df.iloc[0]\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"🏆 OVERALL BEST MODEL\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Model: {best_final_model['Model']} ({best_final_model['Type']})\")\n",
        "print(f\"Log Loss: {best_final_model['Log Loss']:.6f}\")\n",
        "print(f\"ROC-AUC: {best_final_model['ROC-AUC']:.6f}\")\n",
        "print(f\"F1-Score: {best_final_model['F1-Score']:.6f}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Store the best pipeline\n",
        "if best_final_model['Type'] == 'Tuned':\n",
        "    best_final_pipeline = tuned_results[best_final_model['Model']]['pipeline']\n",
        "else:\n",
        "    best_final_pipeline = training_results[best_final_model['Model']]['pipeline']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Final Model Selection and Test Predictions\n",
        "\n",
        "Train the best model on the full training dataset and generate final predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Final Model Training and Prediction\n",
        "\n",
        "print(f\"\\nRefitting best model ({best_final_model['Model']}) on full training data...\")\n",
        "best_final_pipeline.fit(X_train_eng, y_train)\n",
        "\n",
        "print(\"Generating final predictions...\")\n",
        "predictions_df = predict_and_save(best_final_pipeline, X_test_eng, test_df, name=\"Vineet_Burugu\")\n",
        "print(f\"Prediction range: [{predictions_df['pr_CTA'].min():.6f}, {predictions_df['pr_CTA'].max():.6f}]\")\n",
        "print(f\"Mean prediction: {predictions_df['pr_CTA'].mean():.6f}\")\n",
        "print(f\"Std prediction: {predictions_df['pr_CTA'].std():.6f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"✅ FINAL PREDICTIONS SAVED\")\n",
        "print(\"=\"*70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
