{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. What relevant key metrics are provided to evaluate the CTA combinations? And which CTA Copy and CTA Placement did best/worst based on the key metrics? - The main metric provided to evaluate the CTA combinations is click through rate (CTR). This is because the higher the CTR, the more likely the user will click on the CTA and visit the website, which means that this would allow us to evaluate the CTA combinations. Other key metrics are submittedForm, scheduledAppointment, and revenue as these also allow us to evaluate the CTA combinations in terms of what types of clicks happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = train_df.groupby(['ctaCopy', 'ctaPlacement']).agg({\n",
        "    'clickedCTA': 'mean',\n",
        "    'submittedForm': 'mean',\n",
        "    'scheduledAppointment': 'mean',\n",
        "    'revenue': 'mean'\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Displaying Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                      ctaCopy ctaPlacement  clickedCTA  submittedForm  scheduledAppointment    revenue\n",
            "                  Access Your Personalized Mortgage Rates Now       Bottom    0.134821       0.117001              0.051751 218.982609\n",
            "                  Access Your Personalized Mortgage Rates Now       Middle    0.161462       0.126901              0.050671 225.461812\n",
            "                  Access Your Personalized Mortgage Rates Now          Top    0.186482       0.150752              0.054631 221.869852\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate       Bottom    0.153092       0.135631              0.056881 226.882911\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate       Middle    0.169922       0.135811              0.053191 226.945854\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate          Top    0.198452       0.159032              0.054541 225.280528\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes       Bottom    0.154172       0.150122              0.056701 206.301587\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes       Middle    0.183332       0.164792              0.057871 203.102644\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes          Top    0.211753       0.190875              0.060295 210.313433\n"
          ]
        }
      ],
      "source": [
        "print(metrics[['ctaCopy', 'ctaPlacement', 'clickedCTA', 'submittedForm', 'scheduledAppointment', 'revenue']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Performing Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Highest clickedCTA: Get Pre-Approved for a Mortgage in 5 Minutes - Top (0.2118)\n",
            "Highest submittedForm: Get Pre-Approved for a Mortgage in 5 Minutes - Top (0.1909)\n",
            "Highest scheduledAppointment: Get Pre-Approved for a Mortgage in 5 Minutes - Top (0.0603)\n",
            "Highest Revenue: First Time? We've Made it Easy to Find the Best Mortgage Rate - Middle ($226.95)\n"
          ]
        }
      ],
      "source": [
        "best_clicked = metrics.loc[metrics['clickedCTA'].idxmax()]\n",
        "best_submitted = metrics.loc[metrics['submittedForm'].idxmax()]\n",
        "best_appointment = metrics.loc[metrics['scheduledAppointment'].idxmax()]\n",
        "best_revenue = metrics.loc[metrics['revenue'].idxmax()]\n",
        "\n",
        "print(f\"Highest clickedCTA: {best_clicked['ctaCopy']} - {best_clicked['ctaPlacement']} ({best_clicked['clickedCTA']:.4f})\")\n",
        "print(f\"Highest submittedForm: {best_submitted['ctaCopy']} - {best_submitted['ctaPlacement']} ({best_submitted['submittedForm']:.4f})\")\n",
        "print(f\"Highest scheduledAppointment: {best_appointment['ctaCopy']} - {best_appointment['ctaPlacement']} ({best_appointment['scheduledAppointment']:.4f})\")\n",
        "print(f\"Highest Revenue: {best_revenue['ctaCopy']} - {best_revenue['ctaPlacement']} (${best_revenue['revenue']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst Performing Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lowest clickedCTA: Access Your Personalized Mortgage Rates Now - Bottom (0.1348)\n",
            "Lowest submittedForm: Access Your Personalized Mortgage Rates Now - Bottom (0.1170)\n",
            "Lowest scheduledAppointment: Access Your Personalized Mortgage Rates Now - Middle (0.0507)\n",
            "Lowest Revenue: Get Pre-Approved for a Mortgage in 5 Minutes - Middle ($203.10)\n"
          ]
        }
      ],
      "source": [
        "worst_clicked = metrics.loc[metrics['clickedCTA'].idxmin()]\n",
        "worst_submitted = metrics.loc[metrics['submittedForm'].idxmin()]\n",
        "worst_appointment = metrics.loc[metrics['scheduledAppointment'].idxmin()]\n",
        "worst_revenue = metrics.loc[metrics['revenue'].idxmin()]\n",
        "\n",
        "print(f\"Lowest clickedCTA: {worst_clicked['ctaCopy']} - {worst_clicked['ctaPlacement']} ({worst_clicked['clickedCTA']:.4f})\")\n",
        "print(f\"Lowest submittedForm: {worst_submitted['ctaCopy']} - {worst_submitted['ctaPlacement']} ({worst_submitted['submittedForm']:.4f})\")\n",
        "print(f\"Lowest scheduledAppointment: {worst_appointment['ctaCopy']} - {worst_appointment['ctaPlacement']} ({worst_appointment['scheduledAppointment']:.4f})\")\n",
        "print(f\"Lowest Revenue: {worst_revenue['ctaCopy']} - {worst_revenue['ctaPlacement']} (${worst_revenue['revenue']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Which groups of people tend to be more correlated or less correlated with our key metrics?\n",
        "\n",
        "3. What ways can you manipulate the columns/dataset to create features that increase predictive power towards our key metric?\n",
        "\n",
        "4. Besides Log Loss, what other metrics will you use to evaluate the model's performance, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"Load train and test data from current directory.\"\"\"\n",
        "    train_path = 'train.csv'\n",
        "    test_path = 'test.csv'\n",
        "    \n",
        "    if not os.path.exists(train_path):\n",
        "        raise FileNotFoundError(f\"Training file '{train_path}' not found\")\n",
        "    if not os.path.exists(test_path):\n",
        "        raise FileNotFoundError(f\"Test file '{test_path}' not found\")\n",
        "    \n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    \n",
        "    print(f\"Loaded train data: {train_df.shape}\")\n",
        "    print(f\"Loaded test data: {test_df.shape}\")\n",
        "    \n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Building Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Build the preprocessing and modeling pipeline.\"\"\"\n",
        "    \n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=transformers,\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=None))\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred_proba):\n",
        "    \"\"\"Calculate and print evaluation metrics.\"\"\"\n",
        "    logloss = log_loss(y_true, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    brier = brier_score_loss(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"Log Loss: {logloss:.6f} | ROC-AUC: {roc_auc:.6f} | Brier Score: {brier:.6f}\")\n",
        "    return logloss, roc_auc, brier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction and Saving Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_and_save(pipeline, X_test, test_df_original, name=\"Vineet_Burugu\"):\n",
        "    \"\"\"Generate predictions and save to CSV.\"\"\"\n",
        "    os.makedirs('./outputs', exist_ok=True)\n",
        "    output_path = f'./outputs/{name}_predictions.csv'\n",
        "    \n",
        "    pr_CTA = pipeline.predict_proba(X_test)[:, 1]\n",
        "    predictions_df = pd.DataFrame({\n",
        "        'userId': test_df_original['userId'].values,\n",
        "        'pr_CTA': pr_CTA\n",
        "    })\n",
        "    \n",
        "    predictions_df.to_csv(output_path, index=False)\n",
        "    print(f\"Predictions saved to: {output_path}\")\n",
        "    return predictions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded train data: (100000, 18)\n",
            "Loaded test data: (20000, 17)\n"
          ]
        }
      ],
      "source": [
        "train_df, test_df = load_data()\n",
        "\n",
        "feature_cols = [\n",
        "    'ctaCopy', 'ctaPlacement', 'sessionReferrer', 'browser', \n",
        "    'deviceType', 'estimatedAnnualIncome', 'estimatedPropertyType', \n",
        "    'visitCount', 'pageURL', 'scrollDepth', 'editorialSnippet'\n",
        "]\n",
        "\n",
        "available_features = [col for col in feature_cols if col in train_df.columns]\n",
        "\n",
        "X_train = train_df[available_features].copy()\n",
        "y_train = train_df['clickedCTA'].copy()\n",
        "X_test = test_df[available_features].copy()\n",
        "\n",
        "if 'editorialSnippet' in X_train.columns:\n",
        "    X_train['editorialSnippet'] = X_train['editorialSnippet'].astype(str).str.len()\n",
        "    X_test['editorialSnippet'] = X_test['editorialSnippet'].astype(str).str.len()\n",
        "\n",
        "categorical_features = [f for f in ['ctaCopy', 'ctaPlacement', 'sessionReferrer', 'browser', \n",
        "                                    'deviceType', 'estimatedPropertyType', 'pageURL'] \n",
        "                        if f in available_features]\n",
        "\n",
        "numeric_features = [f for f in ['estimatedAnnualIncome', 'visitCount', 'scrollDepth'] \n",
        "                    if f in available_features]\n",
        "\n",
        "if 'editorialSnippet' in available_features:\n",
        "    numeric_features.append('editorialSnippet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Log Loss: 0.422296 | ROC-AUC: 0.700934 | Brier Score: 0.133458\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.4222961053243016, 0.7009335232790259, 0.13345798393221112)"
            ]
          },
          "execution_count": 161,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline = build_pipeline(categorical_features, numeric_features)\n",
        "pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "y_val_pred_proba = pipeline.predict_proba(X_val_split)[:, 1]\n",
        "evaluate(y_val_split, y_val_pred_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions saved to: ./outputs/Vineet_Burugu_predictions.csv\n",
            "Prediction range: [0.006222, 0.612568]\n"
          ]
        }
      ],
      "source": [
        "pipeline.fit(X_train, y_train)\n",
        "predictions_df = predict_and_save(pipeline, X_test, test_df, name=\"Vineet_Burugu\")\n",
        "print(f\"Prediction range: [{predictions_df['pr_CTA'].min():.6f}, {predictions_df['pr_CTA'].max():.6f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Iteration 1: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Model Comparison: Logistic Regression vs HistGradientBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "def build_tree_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Build pipeline for HistGradientBoostingClassifier (no scaling needed).\"\"\"\n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median'))\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=transformers,\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', HistGradientBoostingClassifier(\n",
        "            max_iter=200,\n",
        "            early_stopping=True,\n",
        "            validation_fraction=0.1,\n",
        "            n_iter_no_change=10,\n",
        "            random_state=42,\n",
        "            class_weight=None\n",
        "        ))\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Comparing Both Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Logistic Regression...\n",
            "Training HistGradientBoostingClassifier...\n",
            "\n",
            "============================================================\n",
            "VALIDATION METRICS COMPARISON\n",
            "============================================================\n",
            "\n",
            "Logistic Regression:\n",
            "Log Loss: 0.422296 | ROC-AUC: 0.700934 | Brier Score: 0.133458\n",
            "\n",
            "HistGradientBoostingClassifier:\n",
            "Log Loss: 0.385112 | ROC-AUC: 0.761368 | Brier Score: 0.125093\n",
            "\n",
            "============================================================\n",
            "Winner: HistGradientBoostingClassifier (Log Loss: 0.385112)\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "logistic_pipeline = build_pipeline(categorical_features, numeric_features)\n",
        "tree_pipeline = build_tree_pipeline(categorical_features, numeric_features)\n",
        "\n",
        "print(\"Training Logistic Regression...\")\n",
        "logistic_pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "print(\"Training HistGradientBoostingClassifier...\")\n",
        "tree_pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION METRICS COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_val_logistic = logistic_pipeline.predict_proba(X_val_split)[:, 1]\n",
        "y_val_tree = tree_pipeline.predict_proba(X_val_split)[:, 1]\n",
        "\n",
        "print(\"\\nLogistic Regression:\")\n",
        "log_loss_lr, roc_auc_lr, brier_lr = evaluate(y_val_split, y_val_logistic)\n",
        "\n",
        "print(\"\\nHistGradientBoostingClassifier:\")\n",
        "log_loss_tree, roc_auc_tree, brier_tree = evaluate(y_val_split, y_val_tree)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if log_loss_lr < log_loss_tree:\n",
        "    winner = \"Logistic Regression\"\n",
        "    winner_pipeline = logistic_pipeline\n",
        "    print(f\"Winner: {winner} (Log Loss: {log_loss_lr:.6f})\")\n",
        "else:\n",
        "    winner = \"HistGradientBoostingClassifier\"\n",
        "    winner_pipeline = tree_pipeline\n",
        "    print(f\"Winner: {winner} (Log Loss: {log_loss_tree:.6f})\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Iteration 2: Model Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Feature Engineering: Interaction Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_engineered_features(df):\n",
        "    \"\"\"Create engineered features for heterogeneous CTA responses.\"\"\"\n",
        "    df = df.copy()\n",
        "    \n",
        "    if 'visitCount' in df.columns:\n",
        "        df['is_new_user'] = (df['visitCount'] == 0).astype(int)\n",
        "        df['visit_bucket'] = pd.cut(df['visitCount'], \n",
        "                                    bins=[-1, 0, 2, 5, float('inf')],\n",
        "                                    labels=['0', '1-2', '3-5', '6+']).astype(str)\n",
        "    \n",
        "    if 'scrollDepth' in df.columns:\n",
        "        df['scroll_bucket'] = pd.cut(df['scrollDepth'],\n",
        "                                     bins=[-1, 0, 25, 50, 75, 100],\n",
        "                                     labels=['0', '1-25', '26-50', '51-75', '76-100']).astype(str)\n",
        "    \n",
        "    if 'ctaPlacement' in df.columns and 'scroll_bucket' in df.columns:\n",
        "        df['placement_x_scroll'] = df['ctaPlacement'] + \"_\" + df['scroll_bucket']\n",
        "    \n",
        "    if 'deviceType' in df.columns and 'ctaPlacement' in df.columns:\n",
        "        df['device_x_placement'] = df['deviceType'] + \"_\" + df['ctaPlacement']\n",
        "    \n",
        "    if 'sessionReferrer' in df.columns and 'ctaCopy' in df.columns:\n",
        "        df['referrer_x_copy'] = df['sessionReferrer'] + \"_\" + df['ctaCopy']\n",
        "    \n",
        "    return df\n",
        "\n",
        "X_train_eng = create_engineered_features(X_train)\n",
        "X_test_eng = create_engineered_features(X_test)\n",
        "\n",
        "X_train_split_eng = create_engineered_features(X_train_split)\n",
        "X_val_split_eng = create_engineered_features(X_val_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Updated Feature Lists with Engineered Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_features_eng = categorical_features.copy()\n",
        "categorical_features_eng.extend(['visit_bucket', 'scroll_bucket', 'placement_x_scroll', \n",
        "                                 'device_x_placement', 'referrer_x_copy'])\n",
        "\n",
        "numeric_features_eng = numeric_features.copy()\n",
        "numeric_features_eng.append('is_new_user')\n",
        "\n",
        "if 'visitCount' in numeric_features_eng:\n",
        "    numeric_features_eng.remove('visitCount')\n",
        "if 'scrollDepth' in numeric_features_eng:\n",
        "    numeric_features_eng.remove('scrollDepth')\n",
        "\n",
        "print(f\"Categorical features: {len(categorical_features_eng)}\")\n",
        "print(f\"Numeric features: {len(numeric_features_eng)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Baseline Metrics (Before Feature Engineering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Baseline metrics from Iteration 1 (HistGradientBoostingClassifier):\")\n",
        "print(f\"Log Loss: {log_loss_tree:.6f} | ROC-AUC: {roc_auc_tree:.6f} | Brier Score: {brier_tree:.6f}\")\n",
        "baseline_metrics = {'log_loss': log_loss_tree, 'roc_auc': roc_auc_tree, 'brier': brier_tree}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retraining Models with Engineered Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "logistic_pipeline_eng = build_pipeline(categorical_features_eng, numeric_features_eng)\n",
        "tree_pipeline_eng = build_tree_pipeline(categorical_features_eng, numeric_features_eng)\n",
        "\n",
        "print(\"Training Logistic Regression with engineered features...\")\n",
        "logistic_pipeline_eng.fit(X_train_split_eng, y_train_split)\n",
        "\n",
        "print(\"Training HistGradientBoostingClassifier with engineered features...\")\n",
        "tree_pipeline_eng.fit(X_train_split_eng, y_train_split)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VALIDATION METRICS WITH ENGINEERED FEATURES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "y_val_logistic_eng = logistic_pipeline_eng.predict_proba(X_val_split_eng)[:, 1]\n",
        "y_val_tree_eng = tree_pipeline_eng.predict_proba(X_val_split_eng)[:, 1]\n",
        "\n",
        "print(\"\\nLogistic Regression:\")\n",
        "log_loss_lr_eng, roc_auc_lr_eng, brier_lr_eng = evaluate(y_val_split, y_val_logistic_eng)\n",
        "\n",
        "print(\"\\nHistGradientBoostingClassifier:\")\n",
        "log_loss_tree_eng, roc_auc_tree_eng, brier_tree_eng = evaluate(y_val_split, y_val_tree_eng)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "if log_loss_lr_eng < log_loss_tree_eng:\n",
        "    winner_eng = \"Logistic Regression\"\n",
        "    winner_pipeline_eng = logistic_pipeline_eng\n",
        "    winner_metrics_eng = {'log_loss': log_loss_lr_eng, 'roc_auc': roc_auc_lr_eng, 'brier': brier_lr_eng}\n",
        "    print(f\"Winner: {winner_eng} (Log Loss: {log_loss_lr_eng:.6f})\")\n",
        "else:\n",
        "    winner_eng = \"HistGradientBoostingClassifier\"\n",
        "    winner_pipeline_eng = tree_pipeline_eng\n",
        "    winner_metrics_eng = {'log_loss': log_loss_tree_eng, 'roc_auc': roc_auc_tree_eng, 'brier': brier_tree_eng}\n",
        "    print(f\"Winner: {winner_eng} (Log Loss: {log_loss_tree_eng:.6f})\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ablation Summary: Before vs After Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ABLATION SUMMARY: HistGradientBoostingClassifier\")\n",
        "print(\"=\"*60)\n",
        "print(f\"{'Metric':<15} {'Before':<15} {'After':<15} {'Improvement':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "log_loss_improve = baseline_metrics['log_loss'] - winner_metrics_eng['log_loss']\n",
        "roc_auc_improve = winner_metrics_eng['roc_auc'] - baseline_metrics['roc_auc']\n",
        "brier_improve = baseline_metrics['brier'] - winner_metrics_eng['brier']\n",
        "\n",
        "print(f\"{'Log Loss':<15} {baseline_metrics['log_loss']:<15.6f} {winner_metrics_eng['log_loss']:<15.6f} {log_loss_improve:+.6f}\")\n",
        "print(f\"{'ROC-AUC':<15} {baseline_metrics['roc_auc']:<15.6f} {winner_metrics_eng['roc_auc']:<15.6f} {roc_auc_improve:+.6f}\")\n",
        "print(f\"{'Brier Score':<15} {baseline_metrics['brier']:<15.6f} {winner_metrics_eng['brier']:<15.6f} {brier_improve:+.6f}\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Refitting {winner_eng} on full training data with engineered features...\")\n",
        "winner_pipeline_eng.fit(X_train_eng, y_train)\n",
        "\n",
        "predictions_df = predict_and_save(winner_pipeline_eng, X_test_eng, test_df, name=\"Vineet_Burugu\")\n",
        "print(f\"Prediction range: [{predictions_df['pr_CTA'].min():.6f}, {predictions_df['pr_CTA'].max():.6f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
