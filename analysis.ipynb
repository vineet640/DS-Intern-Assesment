{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Business Understanding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. What relevant key metrics are provided to evaluate the CTA combinations? And which CTA Copy and CTA Placement did best/worst based on the key metrics? - The main metric provided to evaluate the CTA combinations is click through rate (CTR). This is because the higher the CTR, the more likely the user will click on the CTA and visit the website, which means that this would allow us to evaluate the CTA combinations. Other key metrics are submittedForm, scheduledAppointment, and revenue as these also allow us to evaluate the CTA combinations in terms of what types of clicks happen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Computing Metrics\n",
        "\n",
        "Converting columns to numeric to handle any type issues:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics = train_df.groupby(['ctaCopy', 'ctaPlacement']).agg({\n",
        "    'clickedCTA': 'mean',\n",
        "    'submittedForm': 'mean',\n",
        "    'scheduledAppointment': 'mean',\n",
        "    'revenue': 'mean'\n",
        "}).reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Displaying Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for each CTA combination:\n",
            "\n",
            "                                                      ctaCopy ctaPlacement  clickedCTA  submittedForm  scheduledAppointment    revenue\n",
            "                  Access Your Personalized Mortgage Rates Now       Bottom    0.134821       0.117001              0.051751 218.982609\n",
            "                  Access Your Personalized Mortgage Rates Now       Middle    0.161462       0.126901              0.050671 225.461812\n",
            "                  Access Your Personalized Mortgage Rates Now          Top    0.186482       0.150752              0.054631 221.869852\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate       Bottom    0.153092       0.135631              0.056881 226.882911\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate       Middle    0.169922       0.135811              0.053191 226.945854\n",
            "First Time? We've Made it Easy to Find the Best Mortgage Rate          Top    0.198452       0.159032              0.054541 225.280528\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes       Bottom    0.154172       0.150122              0.056701 206.301587\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes       Middle    0.183332       0.164792              0.057871 203.102644\n",
            "                 Get Pre-Approved for a Mortgage in 5 Minutes          Top    0.211753       0.190875              0.060295 210.313433\n"
          ]
        }
      ],
      "source": [
        "print(\"Metrics for each CTA combination:\\n\")\n",
        "print(metrics[['ctaCopy', 'ctaPlacement', 'clickedCTA', 'submittedForm', 'scheduledAppointment', 'revenue']].to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Best Performing Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Highest clickedCTA: Get Pre-Approved for a Mortgage in 5 Minutes - Top\n",
            "clickedCTA: 0.2118\n",
            "\n",
            "Highest submittedForm: Get Pre-Approved for a Mortgage in 5 Minutes - Top\n",
            "submittedForm: 0.1909\n",
            "\n",
            "Highest scheduledAppointment: Get Pre-Approved for a Mortgage in 5 Minutes - Top\n",
            "scheduledAppointment: 0.0603\n",
            "\n",
            "Highest Revenue: First Time? We've Made it Easy to Find the Best Mortgage Rate - Middle\n",
            "Revenue: $226.95\n"
          ]
        }
      ],
      "source": [
        "best_clicked_idx = metrics['clickedCTA'].idxmax()\n",
        "best_submitted_idx = metrics['submittedForm'].idxmax()\n",
        "best_appointment_idx = metrics['scheduledAppointment'].idxmax()\n",
        "best_revenue_idx = metrics['revenue'].idxmax()\n",
        "\n",
        "best_clicked = metrics.loc[best_clicked_idx]\n",
        "best_submitted = metrics.loc[best_submitted_idx]\n",
        "best_appointment = metrics.loc[best_appointment_idx]\n",
        "best_revenue = metrics.loc[best_revenue_idx]\n",
        "\n",
        "print(f\"\\nHighest clickedCTA: {best_clicked['ctaCopy']} - {best_clicked['ctaPlacement']}\")\n",
        "print(f\"clickedCTA: {best_clicked['clickedCTA']:.4f}\")\n",
        "\n",
        "print(f\"\\nHighest submittedForm: {best_submitted['ctaCopy']} - {best_submitted['ctaPlacement']}\")\n",
        "print(f\"submittedForm: {best_submitted['submittedForm']:.4f}\")\n",
        "\n",
        "print(f\"\\nHighest scheduledAppointment: {best_appointment['ctaCopy']} - {best_appointment['ctaPlacement']}\")\n",
        "print(f\"scheduledAppointment: {best_appointment['scheduledAppointment']:.4f}\")\n",
        "\n",
        "print(f\"\\nHighest Revenue: {best_revenue['ctaCopy']} - {best_revenue['ctaPlacement']}\")\n",
        "print(f\"Revenue: ${best_revenue['revenue']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Worst Performing Combinations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lowest clickedCTA: Access Your Personalized Mortgage Rates Now - Bottom\n",
            "clickedCTA: 0.1348\n",
            "\n",
            "Lowest submittedForm: Access Your Personalized Mortgage Rates Now - Bottom\n",
            "submittedForm: 0.1170\n",
            "\n",
            "Lowest scheduledAppointment: Access Your Personalized Mortgage Rates Now - Middle\n",
            "scheduledAppointment: 0.0507\n",
            "\n",
            "Lowest Revenue: Get Pre-Approved for a Mortgage in 5 Minutes - Middle\n",
            "Revenue: $203.10\n"
          ]
        }
      ],
      "source": [
        "worst_clicked_idx = metrics['clickedCTA'].idxmin()\n",
        "worst_submitted_idx = metrics['submittedForm'].idxmin()\n",
        "worst_appointment_idx = metrics['scheduledAppointment'].idxmin()\n",
        "worst_revenue_idx = metrics['revenue'].idxmin()\n",
        "\n",
        "worst_clicked = metrics.loc[worst_clicked_idx]\n",
        "worst_submitted = metrics.loc[worst_submitted_idx]\n",
        "worst_appointment = metrics.loc[worst_appointment_idx]\n",
        "worst_revenue = metrics.loc[worst_revenue_idx]\n",
        "\n",
        "print(f\"\\nLowest clickedCTA: {worst_clicked['ctaCopy']} - {worst_clicked['ctaPlacement']}\")\n",
        "print(f\"clickedCTA: {worst_clicked['clickedCTA']:.4f}\")\n",
        "\n",
        "print(f\"\\nLowest submittedForm: {worst_submitted['ctaCopy']} - {worst_submitted['ctaPlacement']}\")\n",
        "print(f\"submittedForm: {worst_submitted['submittedForm']:.4f}\")\n",
        "\n",
        "print(f\"\\nLowest scheduledAppointment: {worst_appointment['ctaCopy']} - {worst_appointment['ctaPlacement']}\")\n",
        "print(f\"scheduledAppointment: {worst_appointment['scheduledAppointment']:.4f}\")\n",
        "\n",
        "print(f\"\\nLowest Revenue: {worst_revenue['ctaCopy']} - {worst_revenue['ctaPlacement']}\")\n",
        "print(f\"Revenue: ${worst_revenue['revenue']:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Which groups of people tend to be more correlated or less correlated with our key metrics?\n",
        "\n",
        "3. What ways can you manipulate the columns/dataset to create features that increase predictive power towards our key metric?\n",
        "\n",
        "4. Besides Log Loss, what other metrics will you use to evaluate the model's performance, and why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Baseline Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import log_loss, roc_auc_score, brier_score_loss\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    \"\"\"Load train and test data from current directory.\"\"\"\n",
        "    train_path = 'train.csv'\n",
        "    test_path = 'test.csv'\n",
        "    \n",
        "    if not os.path.exists(train_path):\n",
        "        raise FileNotFoundError(f\"Training file '{train_path}' not found\")\n",
        "    if not os.path.exists(test_path):\n",
        "        raise FileNotFoundError(f\"Test file '{test_path}' not found\")\n",
        "    \n",
        "    train_df = pd.read_csv(train_path)\n",
        "    test_df = pd.read_csv(test_path)\n",
        "    \n",
        "    print(f\"Loaded train data: {train_df.shape}\")\n",
        "    print(f\"Loaded test data: {test_df.shape}\")\n",
        "    \n",
        "    return train_df, test_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline Building Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_pipeline(categorical_features, numeric_features):\n",
        "    \"\"\"Build the preprocessing and modeling pipeline.\"\"\"\n",
        "    \n",
        "    categorical_transformer = Pipeline([\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "    \n",
        "    numeric_transformer = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "    \n",
        "    transformers = []\n",
        "    if categorical_features:\n",
        "        transformers.append(('cat', categorical_transformer, categorical_features))\n",
        "    if numeric_features:\n",
        "        transformers.append(('num', numeric_transformer, numeric_features))\n",
        "    \n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=transformers,\n",
        "        remainder='drop'\n",
        "    )\n",
        "    \n",
        "    pipeline = Pipeline([\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=2000, n_jobs=-1, class_weight=None))\n",
        "    ])\n",
        "    \n",
        "    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(y_true, y_pred_proba):\n",
        "    \"\"\"Calculate and print evaluation metrics.\"\"\"\n",
        "    logloss = log_loss(y_true, y_pred_proba)\n",
        "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
        "    brier = brier_score_loss(y_true, y_pred_proba)\n",
        "    \n",
        "    print(f\"\\nValidation Metrics:\")\n",
        "    print(f\"  Log Loss: {logloss:.6f}\")\n",
        "    print(f\"  ROC-AUC: {roc_auc:.6f}\")\n",
        "    print(f\"  Brier Score: {brier:.6f}\")\n",
        "    \n",
        "    return logloss, roc_auc, brier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prediction and Saving Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_and_save(pipeline, X_test, test_df_original, name=\"Vineet_Burugu\"):\n",
        "    \"\"\"Generate predictions and save to CSV.\"\"\"\n",
        "    outputs_dir = './outputs'\n",
        "    os.makedirs(outputs_dir, exist_ok=True)\n",
        "    \n",
        "    output_path = os.path.join(outputs_dir, f'{name}_predictions.csv')\n",
        "    \n",
        "    pr_CTA = pipeline.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    predictions_df = pd.DataFrame({\n",
        "        'userId': test_df_original['userId'].values,\n",
        "        'pr_CTA': pr_CTA\n",
        "    })\n",
        "    \n",
        "    predictions_df.to_csv(output_path, index=False)\n",
        "    print(f\"\\nPredictions saved to: {output_path}\")\n",
        "    \n",
        "    return predictions_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "Data directory './data' not found. Please create it and add train.csv and test.csv",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[97], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m      3\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclickedCTA\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target_col \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m train_df\u001b[38;5;241m.\u001b[39mcolumns:\n",
            "Cell \u001b[0;32mIn[93], line 6\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_dir):\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData directory \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not found. Please create it and add train.csv and test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m train_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m test_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: Data directory './data' not found. Please create it and add train.csv and test.csv"
          ]
        }
      ],
      "source": [
        "train_df, test_df = load_data()\n",
        "\n",
        "target_col = 'clickedCTA'\n",
        "if target_col not in train_df.columns:\n",
        "    raise ValueError(f\"Target column '{target_col}' not found in training data\")\n",
        "\n",
        "feature_cols = [\n",
        "    'ctaCopy', 'ctaPlacement', 'sessionReferrer', 'browser', \n",
        "    'deviceType', 'estimatedAnnualIncome', 'estimatedPropertyType', \n",
        "    'visitCount', 'pageURL', 'scrollDepth', 'editorialSnippet'\n",
        "]\n",
        "\n",
        "available_features = [col for col in feature_cols if col in train_df.columns]\n",
        "missing_features = [col for col in feature_cols if col not in train_df.columns]\n",
        "\n",
        "if missing_features:\n",
        "    print(f\"Warning: Missing features: {missing_features}\")\n",
        "\n",
        "X_train = train_df[available_features].copy()\n",
        "y_train = train_df[target_col].copy()\n",
        "X_test = test_df[available_features].copy()\n",
        "\n",
        "if 'editorialSnippet' in X_train.columns:\n",
        "    X_train['editorialSnippet'] = X_train['editorialSnippet'].astype(str).str.len()\n",
        "if 'editorialSnippet' in X_test.columns:\n",
        "    X_test['editorialSnippet'] = X_test['editorialSnippet'].astype(str).str.len()\n",
        "\n",
        "categorical_features = [\n",
        "    'ctaCopy', 'ctaPlacement', 'sessionReferrer', 'browser', \n",
        "    'deviceType', 'estimatedPropertyType', 'pageURL'\n",
        "]\n",
        "categorical_features = [f for f in categorical_features if f in available_features]\n",
        "\n",
        "numeric_features = [\n",
        "    'estimatedAnnualIncome', 'visitCount', 'scrollDepth'\n",
        "]\n",
        "numeric_features = [f for f in numeric_features if f in available_features]\n",
        "\n",
        "if 'editorialSnippet' in available_features:\n",
        "    numeric_features.append('editorialSnippet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train/Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pipeline = build_pipeline(categorical_features, numeric_features)\n",
        "\n",
        "print(\"Training model on training split...\")\n",
        "pipeline.fit(X_train_split, y_train_split)\n",
        "\n",
        "print(\"Evaluating on validation split...\")\n",
        "y_val_pred_proba = pipeline.predict_proba(X_val_split)[:, 1]\n",
        "evaluate(y_val_split, y_val_pred_proba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Final Model Training and Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Retraining on full training data...\")\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "predictions_df = predict_and_save(pipeline, X_test, test_df, name=\"Vineet_Burugu\")\n",
        "\n",
        "print(f\"\\nGenerated {len(predictions_df)} predictions\")\n",
        "print(f\"Prediction range: [{predictions_df['pr_CTA'].min():.6f}, {predictions_df['pr_CTA'].max():.6f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Iteration 1: Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Iteration 2: Model Improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 6. Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 7. Test Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
